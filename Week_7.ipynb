{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "favorite-cassette",
   "metadata": {},
   "source": [
    "# Create a neural network capable of identifying circles\n",
    "And make it run on a GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "studied-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import fuckit\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcdf68f-ac36-41ba-a3eb-607b5ed75760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 10 True 10\n",
      "0.01 10 True 100\n",
      "0.01 10 True 200\n",
      "0.01 10 False 10\n",
      "0.01 10 False 100\n",
      "0.01 10 False 200\n",
      "0.01 25 True 10\n",
      "0.01 25 True 100\n",
      "0.01 25 True 200\n",
      "0.01 25 False 10\n",
      "0.01 25 False 100\n",
      "0.01 25 False 200\n",
      "0.01 50 True 10\n",
      "0.01 50 True 100\n",
      "0.01 50 True 200\n",
      "0.01 50 False 10\n",
      "0.01 50 False 100\n",
      "0.01 50 False 200\n",
      "0.001 10 True 10\n",
      "0.001 10 True 100\n",
      "0.001 10 True 200\n",
      "0.001 10 False 10\n",
      "0.001 10 False 100\n",
      "0.001 10 False 200\n",
      "0.001 25 True 10\n",
      "0.001 25 True 100\n",
      "0.001 25 True 200\n",
      "0.001 25 False 10\n",
      "0.001 25 False 100\n",
      "0.001 25 False 200\n",
      "0.001 50 True 10\n",
      "0.001 50 True 100\n",
      "0.001 50 True 200\n",
      "0.001 50 False 10\n",
      "0.001 50 False 100\n",
      "0.001 50 False 200\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "parameters = dict(\n",
    "    lr = [0.01, 0.001],\n",
    "    batch_size = [10, 25, 50],\n",
    "    shuffle = [True, False],\n",
    "    epochs = [10, 100, 200]\n",
    ")\n",
    "\n",
    "param_values = [value for value in parameters.values()]\n",
    "\n",
    "for lr, batch_size, shuffle, epochs in product(*param_values): # Note: * here unpacks a three-tuple of three-tuples into just three three-tuples\n",
    "    print(lr, batch_size, shuffle, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protected-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 100 # All our circles will be in square this many pixels wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "painful-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circleSlow(x,y,rad,size,mark=1):\n",
    "    \"\"\"\n",
    "    Create a binary 2D matrix representation of a circle\n",
    "    :param x: x coordinate of circle center\n",
    "    :param y: y coordinate of circle center\n",
    "    :param rad: radius of circle\n",
    "    :param size: size of the matrix (size x size)\n",
    "    :return: torch matrix (size x size) with 1s representing the circle\n",
    "    \"\"\"\n",
    "    pic = torch.zeros(size,size)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            dx = i-x\n",
    "            dy = j-y\n",
    "            d2 = (dx*dx + dy*dy)\n",
    "            if abs(d2-(rad * rad)) < 10:\n",
    "                pic[i,j] = mark\n",
    "            else:\n",
    "                pic[i,j] = 0\n",
    "    return pic   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ecfc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bresenham circle\n",
    "@fuckit\n",
    "def circ8(pic, xc, yc, x, y,mark=1):\n",
    "    \"\"\"\n",
    "    Set 8 symmetric pixels for a circle\n",
    "    Note: fuckit decorator will silently ignore any excpetions below, most likely caused by a\n",
    "        poorly trained network attempting to guess a circle that is out of bounds\n",
    "    \"\"\"\n",
    "    pic[xc+x, yc+y] = mark\n",
    "    pic[xc-x, yc+y] = mark\n",
    "    pic[xc+x, yc-y] = mark\n",
    "    pic[xc-x, yc-y] = mark\n",
    "    pic[xc+y, yc+x] = mark\n",
    "    pic[xc-y, yc+x] = mark\n",
    "    pic[xc+y, yc-x] = mark\n",
    "    pic[xc-y, yc-x] = mark\n",
    "\n",
    "def draw_circleFast(xc,yc,radius,size, mark=1):    \n",
    "    \"\"\"\n",
    "    Draw circle using Bresenham algo\n",
    "    \"\"\"\n",
    "    \n",
    "    # make sure type is float32 pytorch tensor\n",
    "    pic = torch.zeros(size,size,dtype=torch.float32)\n",
    "    \n",
    "    x,y = 0,radius\n",
    "    err = 3 - 2 * radius\n",
    "    circ8(pic, xc, yc, x, y, mark)\n",
    "    while y >= x:    \n",
    "        x = x + 1\n",
    "        if err > 0:\n",
    "            y = y - 1\n",
    "            err = err + 4 * (x - y) + 10\n",
    "        else:\n",
    "            err = err + 4 * x + 6\n",
    "        circ8(pic, xc, yc, x, y, mark)\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confidential-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D representation of a circle, radius 10 at (30,30)\n",
    "circ30 = draw_circleFast(30, 30, 10, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "annoying-smooth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circ30 # Output of draw_circle is a 2D array of mostly 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ancient-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c0a5bb0b80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(circ30) # Here's a visual representation of our circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attractive-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels  = 1,\n",
    "                padding      = 0,\n",
    "                out_channels = 32, # may change?\n",
    "                kernel_size  =  5,  # may change\n",
    "                stride       =  3\n",
    "            ),\n",
    "            # image 100 wide, no padding, kernel 5, stride by 3\n",
    "            # first is [0,4], 2nd [3,7], 3rd [6,10], ... 32nd [93,97]\n",
    "            # this in each dim => 32x32 per channel from 100x100 input\n",
    "            #\n",
    "            # 32x32 - each channel recognizes a feature\n",
    "            #\n",
    "            # 32 channels => 32^3\n",
    "            #\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(\n",
    "                in_channels  = 32,\n",
    "                padding      = 0,\n",
    "                out_channels = 5, # may change?\n",
    "                kernel_size  = 9  # may change\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            # 5 is out channels, 32 is in size to last conv2d, -8 for kernel size 9\n",
    "            nn.Linear(5 * (32 - 8) * (32 - 8), 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            #nn.Linear(512, 400000), # idiots were here\n",
    "            #nn.Linear(400000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, our_data):\n",
    "        x = self.net(our_data)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cultural-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(3, 3))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Conv2d(32, 5, kernel_size=(9, 9), stride=(1, 1))\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=2880, out_features=2048, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=3, bias=True)\n",
       "    (9): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net = NeuralNetwork() # Create an instance of our neural network class, name it \"net\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Get the cuda device\n",
    "print(device)\n",
    "test_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28a93535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_used():\n",
    "    \"\"\"GB used by CUDA\"\"\"\n",
    "    print(\"{:.2f} GiB\".format(torch.cuda.memory_allocated() / (1024*1024*1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f373459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03 GiB\n"
     ]
    }
   ],
   "source": [
    "mem_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "liked-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massage circ30 (a 2D array of mostly 0s and some 1s representing a circle) into something we can use\n",
    "net_input = circ30.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-closer",
   "metadata": {},
   "source": [
    "Note: According to https://stackoverflow.com/a/59566009 Conv2D and many other layers expect input conforming to (n_samples, channels, height, width) # e.g., (1000, 1, 224, 224)\n",
    "\n",
    "So the two unsqueezes above simply pad the input with 1 sample dimension and 1 channel dimension [100, 100] -> [1, 1, 100, 100]\n",
    "\n",
    "Also, unsqueeze seems like it's helpful for hacking stuff together early on - it just adds a dimension at the specified index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beneficial-accounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_input.shape # Our network input is of shape [1, 1, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "documented-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = test_net(net_input.to(device)) # Run input through the network - No training being done here, just one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "preceding-islam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape # Show the shape of our network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "higher-fitness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0044, 0.0467, 0.0034]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-amber",
   "metadata": {},
   "source": [
    "# Create Training Data for Classifying Circles\n",
    "### We already have a circle function draw_circle(), so we just need to utilize it to create a bunch of circles matched with their radius and origin\n",
    "Following https://pytorch.org/tutorials/beginner/data_loading_tutorial.html for the below steps on how to add custom data and datasets and manipulate them via a Dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-holocaust",
   "metadata": {},
   "source": [
    "### Create a custom dataset to hold our training data\n",
    "##### Dataset class\n",
    "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods  \n",
    "* \\_\\_len__ so that len(dataset) returns the size of the dataset.  \n",
    "* \\_\\_getitem__ to support the indexing such that dataset[i] can be used to get iith sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "everyday-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleDataset(Dataset):\n",
    "    def __init__(self, num_circles=10000, r_min=5, r_max=20, size=100):\n",
    "        randomlist=[] # List of circles generated somewhat randomly based on position and radius\n",
    "        truthlist=[] # Cooresponding position and radius data for the generated circles\n",
    "        for i in range(num_circles):\n",
    "            radius = random.randint(r_min, r_max)\n",
    "            x = random.randint(radius,size-radius-1)\n",
    "            y = random.randint(radius,size-radius-1)\n",
    "            c = draw_circleFast(x,y,radius,size).unsqueeze(0)\n",
    "            # Recall tensors fed through our network must have a \"channels\" dimension, in this case it's one, so we will use unsqueeze again to add it\n",
    "            # If we had e.g. Red, Green, and Blue data, we would split that into three 100x100 channels\n",
    "            # \"single\" is chosen here because it seems to be the only numpy type that plays nice with torch.double that are used throughout the network\n",
    "            truthlist.append(np.array([x,y,radius], dtype=np.single))\n",
    "            randomlist.append(c)\n",
    "        self.samples = randomlist # Circles\n",
    "        self.truth = truthlist # Attributes of those circles\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self,idx):\n",
    "        sample = {'circle': self.samples[idx], 'attributes': self.truth[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deluxe-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CircleDataset(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-sussex",
   "metadata": {},
   "source": [
    "### Using a dataloader allows us to\n",
    "* Batch the data\n",
    "* Shuffle the data\n",
    "* Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "broken-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_circle_network(network, verbose=False):\n",
    "    radius = random.randint(4, 20)\n",
    "    x = random.randint(radius,100-radius)\n",
    "    y = random.randint(radius,100-radius)\n",
    "    circle = draw_circleFast(x,y,radius,100).unsqueeze(0).unsqueeze(0)\n",
    "    ans = network(circle.to(device))\n",
    "    if verbose:\n",
    "        print(\"For circle at ({}, {}), r={}, net guessed:\\n{}\".format(str(x), str(y), str(radius), str(ans)))\n",
    "    truth = draw_circleFast(x, y, radius, IMG_SIZE, 0.2)\n",
    "    guess = draw_circleFast(int(ans[0][0]), int(ans[0][1]), int(ans[0][2]), IMG_SIZE)\n",
    "    return (truth + guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "comic-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network_splatter(network, num_checks=10, writer=None, verbose=False):\n",
    "    plt.figure()\n",
    "    num_cols = 5\n",
    "    num_rows = int((num_checks / num_cols) + (1 if (num_checks % num_cols) == 0 else 0))\n",
    "    fig = plt.figure(figsize=(20,10))#f, canvas = plt.subplots(num_rows, num_cols)\n",
    "    for check in range(num_checks):\n",
    "        fig.add_subplot(num_rows, num_cols, check + 1) # + 1 because range is 0-indexed but pyplot is 1-indexed\n",
    "        plt.imshow(check_circle_network(network))\n",
    "    # Add this figure to the tensorboard log\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"Network Splatter\", fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "joint-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate, batch_size, shuffle=True, epochs=40):\n",
    "    import time\n",
    "    \n",
    "    net = NeuralNetwork().to(device)\n",
    "    trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
    "        \n",
    "    # SummaryWriter will store metrics to use with TensorBoard\n",
    "    comment = f' batch_size={batch_size} lr={lr} shuffle={shuffle} epochs={epochs}'\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "            \n",
    "    #loss_func = nn.functional.cross_entropy # Cross entropy only works for 1D tensors\n",
    "    #loss_func = nn.L1Loss()\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # initialize empty list to track batch losses\n",
    "    batch_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Reset Iterator\n",
    "        dataiter = iter(trainloader)\n",
    "        \n",
    "        # Keep track of batch number\n",
    "        batch_number = 0\n",
    "        \n",
    "        # Keep track of time \n",
    "        last_time = time.time()\n",
    "        last_epoch = time.time()\n",
    "        \n",
    "        # Keep track of per-epoch stats\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in dataiter:\n",
    "            batch_number += 1\n",
    "            \n",
    "            # Reset Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass through the network\n",
    "            out = net(batch[\"circle\"].to(device))\n",
    "        \n",
    "            # Calculate loss\n",
    "            loss = loss_func(out, batch[\"attributes\"].to(device)) # Compare our results with the truth data\n",
    "            \n",
    "            # Log the loss for TensorBoard analysis\n",
    "            # NOTE: we can track loss per batch, as this statement does, or do it per epoch later\n",
    "            #writer.add_scalar(\"Loss/train\", loss, epoch * len(dataloader) + batch_number)\n",
    "            \n",
    "            # track batch loss\n",
    "            batch_losses.append(loss.item()) # TODO what does batch_losses do?\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "                \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Time things\n",
    "            next_time = time.time()\n",
    "            delta_time = next_time - last_time\n",
    "            last_time = next_time\n",
    "            \n",
    "        # print any stats of the network wrt training progress and loss\n",
    "        # Time things\n",
    "        next_epoch = time.time()\n",
    "        delta_epoch = next_epoch - last_epoch\n",
    "        last_epoch = next_epoch\n",
    "        if ((epoch % 10 == 0) or (epoch % 39 == 0)) and False:\n",
    "            print(\"run_id: {}, Epoch: {}, DeltaTime = {:.2f}, LastLoss = {:.2f}\".format(\n",
    "                str(run_id),\n",
    "                str(epoch), # Epoch\n",
    "                delta_epoch,\n",
    "                epoch_loss)) # What is the loss for this batch\n",
    "        \n",
    "        # Note, \"net\" here is our neural network instance.  It looks like this:\n",
    "            \"\"\"NeuralNetwork(\n",
    "              (net): Sequential(\n",
    "                (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(3, 3))\n",
    "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                (2): Conv2d(32, 5, kernel_size=(9, 9), stride=(1, 1))\n",
    "                (3): Flatten(start_dim=1, end_dim=-1)\n",
    "                (4): Linear(in_features=2880, out_features=2048, bias=True)\n",
    "                (5): ReLU()\n",
    "                (6): Linear(in_features=2048, out_features=512, bias=True)\n",
    "                (7): ReLU()\n",
    "                (8): Linear(in_features=512, out_features=3, bias=True)\n",
    "                (9): ReLU()\n",
    "              )\n",
    "            )\n",
    "            \"\"\"\n",
    "        # epoch stats\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_histogram(\"conv1.bias\",   net.net[0].bias, epoch)\n",
    "        writer.add_histogram(\"conv1.weight\", net.net[0].weight, epoch)\n",
    "        writer.add_histogram(\"conv2.bias\",   net.net[0].bias, epoch)\n",
    "        writer.add_histogram(\"conv2.weight\", net.net[0].weight, epoch)\n",
    "               \n",
    "    # See how the network did.\n",
    "    check_network_splatter(net, writer=writer)\n",
    "    writer.close()\n",
    "    #return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "compatible-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network with LR=0.01 BATCH=10, SHUFFLE=True, EPOCHS=10\n",
      "Training network with LR=0.01 BATCH=10, SHUFFLE=True, EPOCHS=100\n",
      "Training network with LR=0.01 BATCH=10, SHUFFLE=True, EPOCHS=200\n",
      "Training network with LR=0.01 BATCH=10, SHUFFLE=False, EPOCHS=10\n",
      "Training network with LR=0.01 BATCH=10, SHUFFLE=False, EPOCHS=100\n",
      "Training network with LR=0.01 BATCH=10, SHUFFLE=False, EPOCHS=200\n",
      "Training network with LR=0.01 BATCH=25, SHUFFLE=True, EPOCHS=10\n",
      "Training network with LR=0.01 BATCH=25, SHUFFLE=True, EPOCHS=100\n",
      "Training network with LR=0.01 BATCH=25, SHUFFLE=True, EPOCHS=200\n",
      "Training network with LR=0.01 BATCH=25, SHUFFLE=False, EPOCHS=10\n",
      "Training network with LR=0.01 BATCH=25, SHUFFLE=False, EPOCHS=100\n",
      "Training network with LR=0.01 BATCH=25, SHUFFLE=False, EPOCHS=200\n",
      "Training network with LR=0.01 BATCH=50, SHUFFLE=True, EPOCHS=10\n",
      "Training network with LR=0.01 BATCH=50, SHUFFLE=True, EPOCHS=100\n",
      "Training network with LR=0.01 BATCH=50, SHUFFLE=True, EPOCHS=200\n",
      "Training network with LR=0.01 BATCH=50, SHUFFLE=False, EPOCHS=10\n",
      "Training network with LR=0.01 BATCH=50, SHUFFLE=False, EPOCHS=100\n",
      "Training network with LR=0.01 BATCH=50, SHUFFLE=False, EPOCHS=200\n",
      "Training network with LR=0.001 BATCH=10, SHUFFLE=True, EPOCHS=10\n",
      "Training network with LR=0.001 BATCH=10, SHUFFLE=True, EPOCHS=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brend\\AppData\\Local\\Temp/ipykernel_28984/2016529184.py:5: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(figsize=(20,10))#f, canvas = plt.subplots(num_rows, num_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network with LR=0.001 BATCH=10, SHUFFLE=True, EPOCHS=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brend\\AppData\\Local\\Temp/ipykernel_28984/2016529184.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network with LR=0.001 BATCH=10, SHUFFLE=False, EPOCHS=10\n",
      "Training network with LR=0.001 BATCH=10, SHUFFLE=False, EPOCHS=100\n",
      "Training network with LR=0.001 BATCH=10, SHUFFLE=False, EPOCHS=200\n",
      "Training network with LR=0.001 BATCH=25, SHUFFLE=True, EPOCHS=10\n",
      "Training network with LR=0.001 BATCH=25, SHUFFLE=True, EPOCHS=100\n",
      "Training network with LR=0.001 BATCH=25, SHUFFLE=True, EPOCHS=200\n",
      "Training network with LR=0.001 BATCH=25, SHUFFLE=False, EPOCHS=10\n",
      "Training network with LR=0.001 BATCH=25, SHUFFLE=False, EPOCHS=100\n",
      "Training network with LR=0.001 BATCH=25, SHUFFLE=False, EPOCHS=200\n",
      "Training network with LR=0.001 BATCH=50, SHUFFLE=True, EPOCHS=10\n",
      "Training network with LR=0.001 BATCH=50, SHUFFLE=True, EPOCHS=100\n",
      "Training network with LR=0.001 BATCH=50, SHUFFLE=True, EPOCHS=200\n",
      "Training network with LR=0.001 BATCH=50, SHUFFLE=False, EPOCHS=10\n",
      "Training network with LR=0.001 BATCH=50, SHUFFLE=False, EPOCHS=100\n",
      "Training network with LR=0.001 BATCH=50, SHUFFLE=False, EPOCHS=200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for run_id, (lr, batch_size, shuffle, epochs) in enumerate(product(*param_values)):\n",
    "    print(\"Training network with LR={} BATCH={}, SHUFFLE={}, EPOCHS={}\".format(lr, batch_size, str(shuffle), epochs))\n",
    "    train(lr, batch_size, shuffle, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a533607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03 GiB\n"
     ]
    }
   ],
   "source": [
    "mem_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2a12723-7b2c-4bf5-992c-972a684a67f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   27539 KB |  182459 KB |  189656 GB |  189656 GB |\n",
      "|       from large pool |   27136 KB |  181760 KB |  186076 GB |  186076 GB |\n",
      "|       from small pool |     403 KB |    2610 KB |    3580 GB |    3580 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   27539 KB |  182459 KB |  189656 GB |  189656 GB |\n",
      "|       from large pool |   27136 KB |  181760 KB |  186076 GB |  186076 GB |\n",
      "|       from small pool |     403 KB |    2610 KB |    3580 GB |    3580 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  221184 KB |  221184 KB |  221184 KB |       0 B  |\n",
      "|       from large pool |  217088 KB |  217088 KB |  217088 KB |       0 B  |\n",
      "|       from small pool |    4096 KB |    4096 KB |    4096 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   19564 KB |   43878 KB |  106907 GB |  106907 GB |\n",
      "|       from large pool |   17920 KB |   40448 KB |  102586 GB |  102586 GB |\n",
      "|       from small pool |    1644 KB |    3671 KB |    4321 GB |    4321 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      27    |      95    |  125742 K  |  125742 K  |\n",
      "|       from large pool |       2    |      15    |   21576 K  |   21576 K  |\n",
      "|       from small pool |      25    |      83    |  104166 K  |  104166 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      27    |      95    |  125742 K  |  125742 K  |\n",
      "|       from large pool |       2    |      15    |   21576 K  |   21576 K  |\n",
      "|       from small pool |      25    |      83    |  104166 K  |  104166 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      11    |      11    |      11    |       0    |\n",
      "|       from large pool |       9    |       9    |       9    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |      17    |   75228 K  |   75228 K  |\n",
      "|       from large pool |       2    |      10    |   17360 K  |   17360 K  |\n",
      "|       from small pool |       1    |      11    |   57868 K  |   57868 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f15168-1475-4cf9-85d6-150bede2e22c",
   "metadata": {},
   "source": [
    "Run this command from the dirctory containing \"logdir\" to stand up tensorboard log viewer\n",
    "```sh\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "Then go to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800929d-4f65-4f33-92fa-cef2574eba66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
